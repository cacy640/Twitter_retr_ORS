{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "import os\n",
    "import time  # Import the time module\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API keys and access tokens\n",
    "# currently using: ORS1\n",
    "CONSUMER_KEY = \"b0mldnxty1pQh9U86uJRPQfVW\"\n",
    "CONSUMER_SECRET = \"D6n3ZiQn4la3V4eHZ8GGlCT9URTAradZRJ8q1qEywu0DwDGNYa\"\n",
    "ACCESS_TOKEN = \"1691659341630459904-AtinNb373kCdAlVX84zVKZRLXPJmft\"\n",
    "ACCESS_TOKEN_SECRET = \"TDt5KaawLXCy82jpV6TbN5xRVvptlvhyKoQDjSFJJMtxY\"\n",
    "BEARER_TOKEN = \"AAAAAAAAAAAAAAAAAAAAAItHqAEAAAAAuavAR0x0WoQPImhBNT3kLsJIbJ4%3DNvu9rKrH2VsCH6ziuVfhZnT18cCbNbxaiAjRxKp2NA8P250w2i\"\n",
    "\n",
    "# Authenticate with Twitter v2\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "#client = tweepy.Client(BEARER_TOKEN)\n",
    "client = tweepy.Client(bearer_token=BEARER_TOKEN, consumer_key=CONSUMER_KEY, consumer_secret=CONSUMER_SECRET, \n",
    "                    access_token=ACCESS_TOKEN, access_token_secret=ACCESS_TOKEN_SECRET, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Sleeping for 655 seconds.\n",
      "Rate limit exceeded. Sleeping for 650 seconds.\n",
      "Rate limit exceeded. Sleeping for 662 seconds.\n",
      "Rate limit exceeded. Sleeping for 667 seconds.\n",
      "Rate limit exceeded. Sleeping for 662 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Function to fetch tweets and save to a CSV file\n",
    "def fetch_and_save_tweets(qTopic, start_time, end_time, max_results, file_prefix):\n",
    "    data = []\n",
    "    paginator = tweepy.Paginator(\n",
    "        client.search_recent_tweets,\n",
    "        query=qTopic,\n",
    "        start_time=start_time,\n",
    "        end_time=end_time,\n",
    "        max_results=max_results,\n",
    "        expansions=[\"author_id\", \"geo.place_id\"],\n",
    "        tweet_fields=[\"id\", \"text\", \"created_at\", \"lang\", \"context_annotations\", \"entities\"],\n",
    "        place_fields=[\"name\"]\n",
    "    )\n",
    "\n",
    "    for tweet in paginator.flatten(limit=4000):\n",
    "        data.append(tweet)\n",
    "\n",
    "    # Create a DataFrame from the fetched data\n",
    "    df = pd.DataFrame([tweet.data for tweet in data])\n",
    "\n",
    "    # Generate a filename based on the timestamp\n",
    "    timestamp = datetime.now(timezone('UTC')).strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    filename = f'{file_prefix}.csv'\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "# Define your search parameters\n",
    "qTopic = ''' (\"activity area\" OR \"garden\" OR \"cycling path\" OR \"heritage road\" OR \"outdoor track\" OR \"park\" OR \"nature reserve\" OR \"promenade\" OR \"open space\" OR \"pond\" OR \"lake\" OR \"river\" OR \"beach\" OR \"wetland\") lang:en -is:retweet -is:reply -is:quote -(\"Linkin\" OR \"South Park\" OR \"car\") -(Jimin) -(Boyoung) -(hyunmin) -(Jihyo) -(Woojin)\n",
    "'''\n",
    "current_time = datetime.now(timezone('UTC'))\n",
    "\n",
    "date_to_fetch = \"2023-11-03\"  ## Change date\n",
    "\n",
    "for hour in range(24):\n",
    "    # Calculate the start and end times for the current hour\n",
    "    start_time = f\"{date_to_fetch}T{hour:00}:00:01Z\"\n",
    "    end_time = f\"{date_to_fetch}T{hour:00}:59:59Z\"\n",
    "    fetch_and_save_tweets(qTopic, start_time, end_time, max_results=20, file_prefix=f\"tweets_hourly_03NOV_{hour:00}\")  ## Change filename date\n",
    "\n",
    "    # Sleep for a short time to avoid exceeding rate limits\n",
    "    time.sleep(5)  # Sleep for 5 seconds (adjust as needed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "\n",
    "## IMPORTANT\n",
    "## Change file name and path\n",
    "dir_name = \"C:/Users/Jiaxuan_Wang/OneDrive - Singapore University of Technology and Design/ORS/05 Big data/00_data/\"\n",
    "file_name = datetime.now().strftime('globalTweets_%Y%m%d%H%M.csv')\n",
    "\n",
    "df.to_csv(os.path.join(dir_name, file_name), encoding = \"utf-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
